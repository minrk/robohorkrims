{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638ed5af-53f1-417c-a3e8-499e1e59beaa",
   "metadata": {},
   "source": [
    "# Collect tweets for corpus\n",
    "\n",
    "1. Collect tweets from [@horkrimsisms](https://twitter.com/horkrimsisms)\n",
    "2. (cache to avoid downloading all tweets every time)\n",
    "3. produce `tweet-corpus.txt` of tweets to train models\n",
    "4. Use other notebooks ([markov.ipynb](markov.ipynb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c33e0c0-4596-4575-99e6-a3e78758c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials not in repo\n",
    "# defines bearer_token=\"...\"\n",
    "%run creds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7830a-f0fc-4bb6-a66d-4507a5cfafbd",
   "metadata": {},
   "source": [
    "Connect to twitter API using tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b132d6-80e9-4cec-814c-339b46fb2927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "tw = tweepy.Client(bearer_token=bearer_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734f38bf-41fe-4496-99f3-6ab7b7b62565",
   "metadata": {},
   "source": [
    "Get user id for @horkrimsisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3bd8b71-9f6b-4428-a26a-15d036cd135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"horkrimsisms\"\n",
    "\n",
    "r = tw.get_user(username=username)\n",
    "user_id = r.data.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c02bc7a-3eb2-40ee-9de8-4c5fe9f8d184",
   "metadata": {},
   "source": [
    "Download all horkrimsisms tweets (with caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd96d321-5ca4-43cb-bc6f-d7e27053e69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "def _cache_file(user_id: int) -> Path:\n",
    "    cache_dir = Path(f\"cache\")\n",
    "    cache_dir.mkdir(exist_ok=True)\n",
    "    return cache_dir.joinpath(f\"{user_id}.json\")\n",
    "\n",
    "\n",
    "def load_cache(user_id: int) -> list[dict]:\n",
    "    cache_file = _cache_file(user_id)\n",
    "    if not cache_file.exists():\n",
    "        return []\n",
    "    with cache_file.open() as f:\n",
    "        try:\n",
    "            return json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding cache: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "def save_cache(user_id: int, tweets: list):\n",
    "    \"\"\"Save full list of tweets to cache file\"\"\"\n",
    "    cache_file = _cache_file(user_id)\n",
    "    with cache_file.open(\"w\") as f:\n",
    "        json.dump(tweets, f)\n",
    "\n",
    "\n",
    "def get_tweets(user_id: int) -> list:\n",
    "    \"\"\"Collect tweets\n",
    "    \n",
    "    1. load cache\n",
    "    2. retrieve any new tweets\n",
    "    3. save any new tweets to the cache\n",
    "    4. return list of {id: int, text: \"tweet\"}\n",
    "    \"\"\"\n",
    "    cache = load_cache(user_id)\n",
    "    if cache:\n",
    "        since_id = cache[0][\"id\"]\n",
    "    else:\n",
    "        since_id = None\n",
    "\n",
    "    until_id = None\n",
    "\n",
    "    def fetch() -> list:\n",
    "        \"\"\"Download one page of tweets\"\"\"\n",
    "        return tw.get_users_tweets(\n",
    "            user_id,\n",
    "            max_results=100,\n",
    "            exclude=[\"retweets\", \"replies\"],\n",
    "            until_id=until_id,\n",
    "            since_id=since_id,\n",
    "        ).data\n",
    "\n",
    "    to_add = []\n",
    "    new_tweets = fetch()\n",
    "\n",
    "    if not new_tweets:\n",
    "        return cache\n",
    "\n",
    "    while new_tweets:\n",
    "        to_add.extend(new_tweets)\n",
    "        until_id = to_add[-1].id\n",
    "        new_tweets = fetch()\n",
    "\n",
    "    all_tweets = [dict(t) for t in to_add] + cache\n",
    "    save_cache(user_id, all_tweets)\n",
    "    return all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f60102f9-b38c-4d69-b409-cd3cb06312b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "533"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = get_tweets(user_id)\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d65273-4eb5-4fda-9df7-54fcb8af0a55",
   "metadata": {},
   "source": [
    "Most recent 3 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fb5a142-12c7-42cf-8873-395033b89ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1542921191383617539,\n",
       "  'text': 'imagine a dark carousel of various elks'},\n",
       " {'id': 1541821432535154690,\n",
       "  'text': 'there is a unique hunger here for these nuts'},\n",
       " {'id': 1541459891495309314,\n",
       "  'text': \"it's purely about the density of the taint\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d27c2d-6bf6-44e4-8e22-1a5326a71751",
   "metadata": {},
   "source": [
    "Generate corpus file from list of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b2cadea-fbd8-43eb-ad1b-ac1c599f559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = \"HORKRIMS \"\n",
    "END = \" ENDHORKRIMS\\n\\n\"\n",
    "with open(\"tweet-corpus.txt\", \"w\") as f:\n",
    "    for tweet in tweets:\n",
    "        f.write(START + tweet[\"text\"].replace(\"\\n\", \" \") + END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5addd4fe-ec22-46eb-863d-ab13a9be2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tweet-corpus.jsonl\", \"w\") as f:\n",
    "    for tweet in tweets:\n",
    "        f.write(json.dumps({\n",
    "            \"prompt\": \"\",\n",
    "            \"completion\": \" \" + tweet[\"text\"]\n",
    "        }))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71434acb-1459-4676-ba88-14147f7e3703",
   "metadata": {},
   "source": [
    "Now we have our corpus!\n",
    "\n",
    "Follow-up in another notebook to generate tweets.\n",
    "\n",
    "E.g. [markov.ipynb](markov.ipynb) for markov chain,\n",
    "or [gpt-gen.ipynb](gpt-gen.ipynb) for GPT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
